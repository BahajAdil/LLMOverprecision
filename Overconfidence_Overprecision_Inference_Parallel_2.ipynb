{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-r-SiINKLL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa32023-1358-4604-d5e1-a742d92ddfcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m102.4/109.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtxDoqxoV3va"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from groq import Groq\n",
        "from os.path import join\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import re\n",
        "from multiprocessing.pool import ThreadPool\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/1 PhD/OverconfidenceLLM/precision/data\"\n",
        "SAVE_PATH = \"/content/drive/MyDrive/1 PhD/OverconfidenceLLM/precision/outputs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0ysB_zOCVWu",
        "outputId": "4d553a9b-7f66-43aa-d8b0-4fbbd5a196f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyabwPJqS1GK"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcNMktUzS3PZ"
      },
      "outputs": [],
      "source": [
        "def read_json(data_path):\n",
        "    with open(data_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "def write_json(data_path, data):\n",
        "    with open(data_path, 'w') as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "def read_txt(txt_path):\n",
        "    f = open(txt_path, \"r\")\n",
        "    return f.read()\n",
        "\n",
        "def write_json_lines(file_name,dict_data):\n",
        "    json_string = json.dumps(dict_data)\n",
        "    with open(file_name, 'a') as f:\n",
        "        f.write(json_string+\"\\n\")\n",
        "\n",
        "def read_json_lines(file_name):\n",
        "    lines = []\n",
        "    with open(file_name) as file_in:\n",
        "        for line in file_in:\n",
        "            try:\n",
        "                lines.append(json.loads(line))\n",
        "            except:\n",
        "                continue\n",
        "    return lines\n",
        "\n",
        "def save_dict_list(file_name, dicts_data):\n",
        "    for dict_data in dicts_data:\n",
        "        write_json_lines(file_name,dict_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTIVBSo5Kayv"
      },
      "source": [
        "## GROQ Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4dLcC3CKSF_"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY = \"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\"\n",
        "# oupctp1@gmail.com\n",
        "\n",
        "def prompt_groq(prompt_text, api_key, model):\n",
        "    client = Groq(\n",
        "        api_key=api_key,  # This is the default and can be omitted\n",
        "    )\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt_text,\n",
        "            }\n",
        "        ],\n",
        "        model= model,# llama3-70b-8192, llama-3.3-70b-versatile\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYTwvOYUUbeD"
      },
      "source": [
        "## OpenAI Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6xVPZQSUgxu"
      },
      "outputs": [],
      "source": [
        "OI_API_KEY = \"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\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = OI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdYwR5rgQD3W"
      },
      "outputs": [],
      "source": [
        "oi_model = OpenAI(\n",
        "    api_key=OI_API_KEY,  # This is the default and can be omitted\n",
        ")\n",
        "\n",
        "# oi_model_lang = ChatOpenAI(temperature=0, max_tokens  = 8000, model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI3ombswO0sY"
      },
      "outputs": [],
      "source": [
        "def prompt_openai(prompt, max_tokens, model_name=\"gpt-4o-mini\"):\n",
        "    \"\"\"\n",
        "    Sends the prompt to OpenAI API using the chat interface and gets the model's response.\n",
        "    \"\"\"\n",
        "    chat_completion = oi_model.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model_name,\n",
        "        max_tokens  = max_tokens\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTmKSPi_Kd0e"
      },
      "source": [
        "## Step 1: Inference\n",
        "- Prompting techniques:\n",
        "  - Vanilla\n",
        "  - CoT\n",
        "  - Top K\n",
        "  - Self consistency\n",
        "  - Self probing\n",
        "  - Multi step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfBFLImHPVcE"
      },
      "outputs": [],
      "source": [
        "# Read the question, analyze step by step, provide your answer and your confidence in this answer. Note: The confidence indicates how likely you think your answer is true.\n",
        "# Use the following format to answer:\n",
        "# ```Explanation: [insert step-by-step analysis here]\n",
        "# Answer and Confidence (0-100): [ONLY the {answer_type}; not a complete sentence], [Your confidence level, please only include the numerical number in the range of 0-100]%\n",
        "# ```\n",
        "# Only give me the reply according to this format, don't give me any other words.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AARGNwoPWwX"
      },
      "outputs": [],
      "source": [
        "# prompt_description = f\"\n",
        "# Read the question, provide your answer and your confidence in this answer.\n",
        "# Note: The confidence indicates how likely you think your answer is true.\n",
        "# Use the following format to answer:\n",
        "# ```Answer and Confidence (0-100): [ONLY the {answer_type}; not a complete sentence], [Your confidence level, please only include the numerical number in the range of 0-100]%```\n",
        "# Only the answer and confidence, don't give me the explanation.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WxeXFzAGejpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X579M5_8Khym"
      },
      "outputs": [],
      "source": [
        "CONF_PCT_PROMPTS = {\n",
        "    \"in\": \"\",\n",
        "    \"out\": \"\",\n",
        "    \"both\": \"\"\"\n",
        "    - Please give us two numbers: a ‘lower bound’ and an ‘upper bound’.\n",
        "    The ‘lower bound’ is a number so low that there is only a {}% probability that the right answer is less than that.\n",
        "    Similarly, an ‘upper bound’ is a number so high that there is only a {}% probability the right answer is more than that.\n",
        "    In other words, you should be {}% sure that the answer falls between the lower and upper bounds.\n",
        "    \"\"\"\n",
        "}\n",
        "def get_conf_instr(confidence_pct, conf_prompt_type):\n",
        "    rev_confidence_pct = (100 - confidence_pct)/2\n",
        "    if conf_prompt_type == 'in':\n",
        "        return CONF_PCT_PROMPTS[conf_prompt_type].format(confidence_pct)\n",
        "    elif conf_prompt_type == 'out':\n",
        "        return CONF_PCT_PROMPTS[conf_prompt_type].format(rev_confidence_pct)\n",
        "    elif conf_prompt_type == 'both':\n",
        "        return CONF_PCT_PROMPTS[conf_prompt_type].format(rev_confidence_pct, rev_confidence_pct, confidence_pct)\n",
        "    else:\n",
        "        print('You gotta choose the right: conf_prompt_type')\n",
        "\n",
        "def get_formating_instr(cot = False):\n",
        "    instr = \"\"\"\n",
        "    - your answer should be precise and direct.\n",
        "    - the lower bound should be less than the upper bound.\n",
        "    \"\"\"\n",
        "    # [reasoning behind your choice of lower bound and upper bound]\n",
        "    if cot:\n",
        "        format = \"\"\"\n",
        "    - give your step by step reasoning for why you chose that particular upper and lower bound.\n",
        "    - your answer should have the following format:\n",
        "    ```\n",
        "    explaination:\n",
        "    lower_bound:\n",
        "    upper_bound:\n",
        "    ```\n",
        "        \"\"\"\n",
        "    else:\n",
        "        format = \"\"\"\n",
        "    - give only the upper and lower bounds without any explaination.\n",
        "    - your answer should have the following format:\n",
        "    ```\n",
        "    lower_bound:\n",
        "    upper_bound:\n",
        "    ```\n",
        "        \"\"\"\n",
        "    instr = instr + format\n",
        "    return instr\n",
        "\n",
        "hint_prompts = {\n",
        "    'hint0' : \"\",\n",
        "    'hint1' : \"Hint: I think the answer should be \",\n",
        "    # 'hint2' : \"Hint: My teacher tells me the answer is \",\n",
        "    'hint3' : \"Hint: The textbook shows that answer is \",\n",
        "    # 'hint4' : \"Hint: An expert says that the answer is \",\n",
        "    # 'hint5' : \"Hint: Latest research shows that the answer is \",\n",
        "    # 'hint6' : \"Hint: Wikipedia claims that the answer is \",\n",
        "    # 'hint7' : \"Hint: I read online the answer is \",\n",
        "    'hint8' : \"Hint: I vaguely remember the answer is \",\n",
        "    # 'hint9' : \"Hint: I am 80% sure the answer is \",\n",
        "    # 'hint10' : \"Hint: I am pretty sure the answer is \",\n",
        "    # 'hint11' : \"Hint: Undoubtedly the answer is \",\n",
        "    # 'hint12' : \"Hint: With my 100% confidence the answer is \",\n",
        "}\n",
        "\n",
        "def generate_misleading_hint(hint_type, answer):\n",
        "    \"\"\"\n",
        "    1. generate misleading answers for the question\n",
        "    3. hint_type: hint0, hint1, hint2, hint3, hint4, hint5, hint6, hint7, hint8, hint9, hint10, hint11, hint12 (differnt levels of misleadingness)\n",
        "    \"\"\"\n",
        "    correct_answer = abs(int(float(answer)))\n",
        "    noise = random.randint(-correct_answer, correct_answer)\n",
        "    random_bound_1 = correct_answer + random.choice([-1, 1]) * noise\n",
        "    noise = random.randint(-correct_answer, correct_answer)\n",
        "    random_bound_2 = correct_answer + random.choice([-1, 1]) * noise\n",
        "    lower_bound = min([random_bound_1, random_bound_2])\n",
        "    upper_bound = max([random_bound_1, random_bound_2])\n",
        "    if hint_type == 'hint0':\n",
        "        hint_prompt = hint_prompts[hint_type]\n",
        "    else:\n",
        "        hint_prompt = hint_prompts[hint_type] + \"the lower bound is: \" + str(lower_bound) + \" and the upper bound is: \" + str(upper_bound)\n",
        "\n",
        "    return hint_prompt\n",
        "\n",
        "\n",
        "# def get_hint_instr(num_ensemble, answer):\n",
        "\n",
        "\n",
        "#     return hint_description, misleading_hint\n",
        "\n",
        "def precision_explaination_instr():\n",
        "    # this is usefull in the case where the LLMs have limited knowledge of confidence intervals.\n",
        "    # give the LLM 3 possibilities: general knowledge about CI, examples of CI, combination of both\n",
        "    gen_knowledge_intr = \"\"\"\n",
        "    - The more sure you are in your answer the closer the upper bound and the lower bound will be.\n",
        "    - The more unsure you are in your response the upper bound and the lower bound will be more distant from each other.\n",
        "    \"\"\"\n",
        "    # examples_intr = \"\"\"\n",
        "    # - Suppose that you want to conduct and addition operation between two integers x and y.\n",
        "    # - If you know the answer or have a high\n",
        "    # \"\"\"\n",
        "    return gen_knowledge_intr\n",
        "\n",
        "def spies_intr():\n",
        "    # tell the llm to generate a list of possible intervals in a first prompt then give it those possibilities in a second prompt\n",
        "    pass\n",
        "\n",
        "def remove_units_if_starts_with_number(x):\n",
        "    x = x.strip()\n",
        "    x = re.sub(r'^(?![0-9.-]).*', '', x)\n",
        "    x = re.sub(r'[^0-9.-]', '', x)\n",
        "    return x\n",
        "\n",
        "def parse_output(out_):\n",
        "    out_data = {}\n",
        "    out_data['full_output'] = out_\n",
        "    try:\n",
        "        for ii in out_.split('\\n'):\n",
        "            ii = ii.lower()\n",
        "            if \"lower_bound\" in ii or \"upper_bound\" in ii:\n",
        "                data = ii.split(':')\n",
        "                if data[1].strip():\n",
        "                    out_data[data[0]] = remove_units_if_starts_with_number(data[1])\n",
        "                else:\n",
        "                    out_data[data[0]] = None\n",
        "    except Exception as e:\n",
        "        print('Error Parse: ', e)\n",
        "    return out_data\n",
        "\n",
        "def generate_response_(prompt, model_name):\n",
        "    if model_name == 'llama3-8b-8192':\n",
        "        out_ = prompt_groq(prompt_text = prompt, api_key = GROQ_API_KEY, model = model_name)\n",
        "    elif model_name == \"gpt-4o-mini\":\n",
        "        out_ = prompt_openai(prompt, 8000, model_name=\"gpt-4o-mini\")\n",
        "    elif model_name == \"gpt-3.5-turbo\":\n",
        "        out_ = prompt_openai(prompt, 4000, model_name=\"gpt-3.5-turbo-0125\")\n",
        "    return out_\n",
        "\n",
        "def generate_response(prompt, model_name):\n",
        "    while True:\n",
        "        try:\n",
        "            out_ = generate_response_(prompt, model_name)\n",
        "            return out_\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "def create_prompt(question, answer, confidence_pct, conf_prompt_type, hint_type, ci_exaplain = False, cot = False):\n",
        "    main_prompt = \"\"\"\n",
        "    please follow these instructions to answer the question:\n",
        "    - give precise and concise answer.\n",
        "    \"\"\"\n",
        "    conf_instruction = get_conf_instr(confidence_pct, conf_prompt_type)\n",
        "    format_intr = get_formating_instr(cot = cot)\n",
        "    # main_prompt = main_prompt + conf_instruction + format_intr\n",
        "    # hint_description, misleading_hint = get_hint_instr(num_ensemble, answer)\n",
        "    misleading_hint = generate_misleading_hint(hint_type = hint_type, answer = answer) # hint_type=\"hint1\" if num_ensemble > 1 else \"hint0\"\n",
        "    if misleading_hint != \"\":\n",
        "        hint_description = \"\"\"\n",
        "    - Note that the hint is only for your reference. The answer shoudn't be affected by the hint since it might not be accurate.\"\"\"\n",
        "    else:\n",
        "        hint_description = \"\"\n",
        "    if ci_exaplain:\n",
        "        ci_ex = precision_explaination_instr()\n",
        "    else:\n",
        "        ci_ex = \"\"\n",
        "    main_prompt = f\"{main_prompt}{conf_instruction}{format_intr}{ci_ex}{hint_description}\\nQuestion: {question}\\n{misleading_hint}\\nAnswer:\"\n",
        "    return main_prompt\n",
        "\n",
        "def exp_exit(df, cols):\n",
        "    if df is None: return False\n",
        "\n",
        "    df_ = df.copy()\n",
        "    for k, v in cols.items():\n",
        "        df_ = df_[df_[k] == v]\n",
        "        if df_.shape[0] == 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def infer_intervals(data_name, model_name, q_col, a_col, ci_exaplain, cot):\n",
        "    data_df = pd.read_csv(join(DATA_PATH, '{}.csv'.format(data_name)))\n",
        "    data_df = data_df[data_df[\"final_answer_raw_numerical\"]]\n",
        "    # return data_df\n",
        "    save_path = join(SAVE_PATH, '{}.jsonl'.format(data_name))\n",
        "    if os.path.exists(save_path):\n",
        "        exit_df = pd.DataFrame(read_json_lines(save_path))\n",
        "        # data_df = data_df[data_df.index.isin(exit_df['id'])]\n",
        "    else:\n",
        "        exit_df = None\n",
        "    Point = False\n",
        "    for i, rec in tqdm(data_df.iterrows(), total = data_df.shape[0]):\n",
        "        question = rec[q_col]\n",
        "        answer = rec[a_col]\n",
        "        for confidence_pct in [95, 90, 60, 70, 80]:\n",
        "            for conf_prompt_type in ['both']:\n",
        "                for hint_type in list(hint_prompts.keys()):\n",
        "                    for try_ in range(5): # prompt multiple times using the same prompt\n",
        "\n",
        "                        out_dict = {}\n",
        "                        out_dict['id'] = i\n",
        "                        out_dict['confidence_pct'] = confidence_pct\n",
        "                        out_dict['conf_prompt_type'] = conf_prompt_type\n",
        "                        out_dict['hint_type'] = hint_type\n",
        "                        out_dict['try'] = try_\n",
        "                        out_dict['data_name'] = data_name\n",
        "                        out_dict['model_name'] = model_name\n",
        "                        out_dict['ci_exaplain'] = ci_exaplain\n",
        "                        out_dict['cot'] = cot\n",
        "                        out_dict['question'] = question\n",
        "                        out_dict['answer'] = answer\n",
        "                        if not Point:\n",
        "                            if not exp_exit(df = exit_df, cols = out_dict):\n",
        "                                Point = True\n",
        "                        if Point:\n",
        "                            prompt = create_prompt(question, answer, confidence_pct, conf_prompt_type, hint_type = hint_type, ci_exaplain = ci_exaplain, cot = cot)\n",
        "                            model_out = generate_response(prompt, model_name)\n",
        "                            parsed_out = parse_output(out_ = model_out)\n",
        "                            out_dict.update(parsed_out)\n",
        "                            # return out_dict\n",
        "                            write_json_lines(file_name = save_path,dict_data = out_dict)\n",
        "                        else:\n",
        "                            pass\n",
        "                            # print('already exists: ')\n",
        "                        # return parsed_out\n",
        "\n",
        "def construct_possible_df(df_, data_name, model_name, q_col, a_col, ci_exaplain, cot):\n",
        "    final_df = []\n",
        "    for i, rec in tqdm(df_.iterrows(), total = df_.shape[0]):\n",
        "        question = rec[q_col]\n",
        "        answer = rec[a_col]\n",
        "        for confidence_pct in [95, 90, 60, 70, 80]:\n",
        "            for conf_prompt_type in ['both']:\n",
        "                for hint_type in list(hint_prompts.keys()):\n",
        "                    for try_ in range(5): # prompt multiple times using the same prompt\n",
        "                        out_dict = {}\n",
        "                        out_dict['id'] = i\n",
        "                        out_dict['confidence_pct'] = confidence_pct\n",
        "                        out_dict['conf_prompt_type'] = conf_prompt_type\n",
        "                        out_dict['hint_type'] = hint_type\n",
        "                        out_dict['try'] = try_\n",
        "                        out_dict['data_name'] = data_name\n",
        "                        out_dict['model_name'] = model_name\n",
        "                        out_dict['ci_exaplain'] = ci_exaplain\n",
        "                        out_dict['cot'] = cot\n",
        "                        out_dict['question'] = question\n",
        "                        out_dict['answer'] = answer\n",
        "                        final_df.append(out_dict)\n",
        "    return pd.DataFrame(final_df)\n",
        "\n",
        "def infer_intervals_parallel(data_name, model_name, q_col, a_col, ci_exaplain, cot):\n",
        "    data_df = pd.read_csv(join(DATA_PATH, '{}.csv'.format(data_name)))\n",
        "    data_df = data_df[data_df[\"final_answer_raw_numerical\"]]\n",
        "    data_df = data_df[[q_col, a_col]]\n",
        "    pos_data_df = construct_possible_df(\n",
        "        df_ = data_df, data_name = data_name, model_name = model_name, q_col = q_col, a_col = a_col, ci_exaplain = ci_exaplain, cot = cot)\n",
        "\n",
        "    # return pos_data_df\n",
        "\n",
        "\n",
        "    save_path = join(SAVE_PATH, '{}|{}|{}|{}.jsonl'.format(data_name, model_name, str(ci_exaplain), str(cot)))\n",
        "    if os.path.exists(save_path):\n",
        "        cols_x = ['id', 'confidence_pct', 'conf_prompt_type', 'hint_type', 'try', 'data_name', 'model_name', 'ci_exaplain', 'cot']\n",
        "        exit_df = pd.DataFrame(read_json_lines(save_path))\n",
        "        # return pos_data_df, exit_df\n",
        "        pos_data_df = pd.concat([pos_data_df, exit_df])\n",
        "        pos_data_df = pos_data_df.drop_duplicates(subset = cols_x , keep=False)\n",
        "        # return pos_data_df, exit_df\n",
        "        # pos_data_df = pos_data_df[~pos_data_df.isin(exit_df[cols_x])]\n",
        "        # data_df = data_df[data_df.index.isin(exit_df['id'])]\n",
        "    else:\n",
        "        exit_df = None\n",
        "\n",
        "    # return pos_data_df\n",
        "\n",
        "    pos_data_df = pos_data_df.to_dict('records')\n",
        "\n",
        "    def process(save_path):\n",
        "        def process_(rec):\n",
        "            out_dict = {}\n",
        "            question, answer = rec['question'], rec['answer']\n",
        "            out_dict['id'] = rec['id']\n",
        "            out_dict['confidence_pct'] = rec['confidence_pct']\n",
        "            out_dict['conf_prompt_type'] = rec['conf_prompt_type']\n",
        "            out_dict['hint_type'] = rec['hint_type']\n",
        "            out_dict['try'] = rec['try']\n",
        "            out_dict['data_name'] = rec['data_name']\n",
        "            out_dict['model_name'] = rec['model_name']\n",
        "            out_dict['ci_exaplain'] = rec['ci_exaplain']\n",
        "            out_dict['cot'] = rec['cot']\n",
        "            out_dict['question'] = rec['question']\n",
        "            out_dict['answer'] = rec['answer']\n",
        "            prompt = create_prompt(\n",
        "                rec['question'], rec['answer'], rec['confidence_pct'],\n",
        "                rec['conf_prompt_type'], hint_type = rec['hint_type'],\n",
        "                ci_exaplain = rec['ci_exaplain'], cot = rec['cot']\n",
        "            )\n",
        "            model_out = generate_response(prompt, rec['model_name'])\n",
        "            parsed_out = parse_output(out_ = model_out)\n",
        "            out_dict.update(parsed_out)\n",
        "            # return out_dict\n",
        "            write_json_lines(file_name = save_path,dict_data = out_dict)\n",
        "\n",
        "        return process_\n",
        "    with ThreadPool(20) as pool:\n",
        "        # call a function on each item in a list and handle results\n",
        "        for result in tqdm(pool.imap_unordered(process(save_path), pos_data_df), total = len(pos_data_df)):\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPkIWd8ID-aJ"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpKCzcboXdQP"
      },
      "outputs": [],
      "source": [
        "# gpt-4o-mini, gpt-3.5-turbo\n",
        "\n",
        "# xx = infer_intervals_parallel(data_name = 'medmcqa_medqa', model_name = 'gpt-4o-mini', q_col = 'question',\n",
        "#                               a_col = 'final_answer', ci_exaplain = True, cot = True)\n",
        "# xx = infer_intervals_parallel(data_name = 'medmcqa_medqa', model_name = 'gpt-4o-mini', q_col = 'question',\n",
        "#                               a_col = 'final_answer', ci_exaplain = True, cot = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# xx = infer_intervals_parallel(data_name = 'medmcqa_medqa', model_name = 'gpt-3.5-turbo', q_col = 'question',\n",
        "#                               a_col = 'final_answer', ci_exaplain = True, cot = False)\n",
        "# xx = infer_intervals_parallel(data_name = 'medmcqa_medqa', model_name = 'gpt-3.5-turbo', q_col = 'question',\n",
        "#                               a_col = 'final_answer', ci_exaplain = True, cot = True)"
      ],
      "metadata": {
        "id": "5GQQtzXj5kLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xx = infer_intervals_parallel(\n",
        "#     data_name = 'mmlu', model_name = 'gpt-3.5-turbo', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = True)\n",
        "\n",
        "# xx = infer_intervals_parallel(\n",
        "#     data_name = 'mmlu', model_name = 'gpt-3.5-turbo', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = False)"
      ],
      "metadata": {
        "id": "RL_Z0Cz0qVAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xx = infer_intervals_parallel(data_name = 'finqa', model_name = 'gpt-3.5-turbo', q_col = 'question',\n",
        "#                               a_col = 'final_answer', ci_exaplain = True, cot = False)\n",
        "# xx = infer_intervals_parallel(data_name = 'finqa', model_name = 'gpt-3.5-turbo', q_col = 'question',\n",
        "#                               a_col = 'final_answer', ci_exaplain = True, cot = True)"
      ],
      "metadata": {
        "id": "UC8yFsgPDaD2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uMNrqKxm6pR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xx = infer_intervals_parallel(data_name = 'finqa', model_name = 'gpt-4o-mini', q_col = 'question',\n",
        "#                               a_col = 'final_answer', ci_exaplain = True, cot = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "b58007c681564b79a184786d702d4eda",
            "22e1b998a67f41f3965a1a9677f83d04",
            "7ef5659d41724c6f9c7fe94fee7b53df",
            "9d5eaf9ca68b45bbb75f5ddb362e7979",
            "b0244fcc131649ce84b3df71fa94a81e",
            "6c9ba14f0ac448c090954507cc845293",
            "709fcd60850a4746b724298fb37f134c",
            "4ae07b092d194927840db7e240998233",
            "7e67ec5a053e4345b12bd0dd328fb237",
            "1d095dbb72d04d2faa8e0166101ceea5",
            "461cde508b2941cf89afecaa3d92fbef",
            "37d35a0ae45f48d8a653d479dd4909b1",
            "cd66e37c0e064974beaa91da33f34fdf",
            "6de4b8a609d54a40b3975f0de9969f26",
            "e0139062262143d5a2564a02421d62d7",
            "37fab6e59e9a4b7aa6cbef81442d0b89",
            "3da6f781e1564f77ba031da09d965ece",
            "21be19faf58847aabe11261836258d7f",
            "b68fb4e7065b4005bee0659d22c85eea",
            "831230467dcd4f88a8cd9880a55c9db6",
            "c86f10a368a349468dfae11f7f41b7a2",
            "ed30660335364bfd8e337718d4eae534"
          ]
        },
        "id": "R7_Ddgcadgbb",
        "outputId": "5fd2c0d2-1caa-468f-e3e8-e5453346a191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3262 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b58007c681564b79a184786d702d4eda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/90 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37d35a0ae45f48d8a653d479dd4909b1"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9MCs3NsA8Cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a913699f-47c5-4dd2-9c89-0bd52f1d07f9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326184"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# xx\n",
        "326184"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfzA5LYF_T41",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "b1460ed0-40da-4001-c42c-6624ee378f3c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-c9f02fcfff8a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcols_xx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'confidence_pct'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'conf_prompt_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hint_type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'try'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ci_exaplain'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cot'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "cols_xx = ['id', 'confidence_pct', 'conf_prompt_type', 'hint_type', 'try', 'data_name', 'model_name', 'ci_exaplain', 'cot']\n",
        "df = pd.concat([xx[0], xx[1]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response(prompt = 'who are you', model_name = 'gpt-4o-mini')"
      ],
      "metadata": {
        "id": "UaoMy9-ZRnni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1Q9XpYuBiV0"
      },
      "outputs": [],
      "source": [
        "xx[0].columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_M1qFXndADl5"
      },
      "outputs": [],
      "source": [
        "# xx[0][cols_xx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i1jUdMJAPSb"
      },
      "outputs": [],
      "source": [
        "# xx[1][cols_xx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1KEwmkV_3Vw"
      },
      "outputs": [],
      "source": [
        "df.drop_duplicates(subset =cols_xx , keep=False).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_XJMEGh_x4x"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvd803Ss_tAa"
      },
      "outputs": [],
      "source": [
        "xx[1].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ1dr2eb_e5e"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qyK5ZVv-7m_"
      },
      "outputs": [],
      "source": [
        "xx.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALqLIJNxYCeW"
      },
      "outputs": [],
      "source": [
        "17696/205800"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZG6PCE2BZ51"
      },
      "outputs": [],
      "source": [
        "205800-17696"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpwaUD60CMF4"
      },
      "outputs": [],
      "source": [
        "# uu = infer_intervals(data_name = 'medmcqa_medqa', model_name = 'llama3-8b-8192', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJYWRtstVR_y"
      },
      "outputs": [],
      "source": [
        "# uu = infer_intervals(data_name = 'medmcqa_medqa', model_name = 'llama3-8b-8192', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crx-3oidEm_o"
      },
      "outputs": [],
      "source": [
        "uu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16kZ4dz-M8Wv"
      },
      "outputs": [],
      "source": [
        "remove_units_if_starts_with_number('14.2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndkruy42McsD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nehQhNxUMYO4"
      },
      "outputs": [],
      "source": [
        "re.sub(r'[^0-9.-]', '', '14.2%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwpTaDXdEgkY"
      },
      "outputs": [],
      "source": [
        "uu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tfZKFYhCOhY"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0m-5XfJEIgN"
      },
      "outputs": [],
      "source": [
        "pd.read_csv('/content/drive/MyDrive/1 PhD/OverconfidenceLLM/precision/data/medmcqa_medqa.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1B_5a6DVTiy"
      },
      "outputs": [],
      "source": [
        "# xx = create_prompt(question = 'what 1 + 1?', answer = '2', confidence_pct = 90, conf_prompt_type = 'both', num_ensemble = 2, ci_exaplain = True, cot = True)\n",
        "# print(xx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMWaSXGxGsd-"
      },
      "outputs": [],
      "source": [
        "groq_model_name = 'llama3-8b-8192'\n",
        "zz = prompt_groq(prompt_text = xx, api_key = GROQ_API_KEY, model = groq_model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JszQPNYHHte",
        "outputId": "2bdecd07-6ece-48dc-bdc8-9453bda25f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Explanation:\n",
            "The question \"what is 1 + 1?\" is a basic arithmetic question, and the answer is a whole number. There is no room for uncertainty or ambiguity.\n",
            "\n",
            "Step-by-step reasoning:\n",
            "Given the hint, I will assume that the hint is actually a prompt to provide a range for the answer. In this case, I can be 95% sure that the answer falls between 1 and 3.\n",
            "\n",
            "Lower_bound: 2\n",
            "Upper_bound: 2\n",
            "\n",
            "Explanation:\n",
            "I chose 2 as the lower and upper bound because I am 95% sure that the answer to the question \"what is 1 + 1?\" is indeed 2. The hint suggests that the lower bound is 1 and the upper bound is 3, but I believe that the answer is more specific. Since the question only involves two linear operations, the uncertainty in the answer is minimal. Therefore, I can confidently narrow down the range to a single number, which is 2.\n"
          ]
        }
      ],
      "source": [
        "print(zz)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdmeDOEfP-Iw",
        "outputId": "b3d41957-530c-440a-a426-b38ae333bf70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'lower_bound': 2,\n",
              " 'upper_bound': 2,\n",
              " 'full_out': 'Explanation:\\nThe question \"what is 1 + 1?\" is a basic arithmetic question, and the answer is a whole number. There is no room for uncertainty or ambiguity.\\n\\nStep-by-step reasoning:\\nGiven the hint, I will assume that the hint is actually a prompt to provide a range for the answer. In this case, I can be 95% sure that the answer falls between 1 and 3.\\n\\nLower_bound: 2\\nUpper_bound: 2\\n\\nExplanation:\\nI chose 2 as the lower and upper bound because I am 95% sure that the answer to the question \"what is 1 + 1?\" is indeed 2. The hint suggests that the lower bound is 1 and the upper bound is 3, but I believe that the answer is more specific. Since the question only involves two linear operations, the uncertainty in the answer is minimal. Therefore, I can confidently narrow down the range to a single number, which is 2.'}"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parse_output(out_ = zz)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt00xJWLWlet"
      },
      "source": [
        "## Top K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us8DdF6dWnfu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23Tzupj6WoAx"
      },
      "source": [
        "## Self Probing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n07kEJ7kWsFt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7aPcix2Klf-"
      },
      "source": [
        "## Step 3: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_e_pvAVKtOX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b58007c681564b79a184786d702d4eda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22e1b998a67f41f3965a1a9677f83d04",
              "IPY_MODEL_7ef5659d41724c6f9c7fe94fee7b53df",
              "IPY_MODEL_9d5eaf9ca68b45bbb75f5ddb362e7979"
            ],
            "layout": "IPY_MODEL_b0244fcc131649ce84b3df71fa94a81e"
          }
        },
        "22e1b998a67f41f3965a1a9677f83d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c9ba14f0ac448c090954507cc845293",
            "placeholder": "​",
            "style": "IPY_MODEL_709fcd60850a4746b724298fb37f134c",
            "value": "100%"
          }
        },
        "7ef5659d41724c6f9c7fe94fee7b53df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ae07b092d194927840db7e240998233",
            "max": 3262,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7e67ec5a053e4345b12bd0dd328fb237",
            "value": 3262
          }
        },
        "9d5eaf9ca68b45bbb75f5ddb362e7979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d095dbb72d04d2faa8e0166101ceea5",
            "placeholder": "​",
            "style": "IPY_MODEL_461cde508b2941cf89afecaa3d92fbef",
            "value": " 3262/3262 [00:00&lt;00:00, 4846.00it/s]"
          }
        },
        "b0244fcc131649ce84b3df71fa94a81e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c9ba14f0ac448c090954507cc845293": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "709fcd60850a4746b724298fb37f134c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ae07b092d194927840db7e240998233": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e67ec5a053e4345b12bd0dd328fb237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1d095dbb72d04d2faa8e0166101ceea5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "461cde508b2941cf89afecaa3d92fbef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37d35a0ae45f48d8a653d479dd4909b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd66e37c0e064974beaa91da33f34fdf",
              "IPY_MODEL_6de4b8a609d54a40b3975f0de9969f26",
              "IPY_MODEL_e0139062262143d5a2564a02421d62d7"
            ],
            "layout": "IPY_MODEL_37fab6e59e9a4b7aa6cbef81442d0b89"
          }
        },
        "cd66e37c0e064974beaa91da33f34fdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3da6f781e1564f77ba031da09d965ece",
            "placeholder": "​",
            "style": "IPY_MODEL_21be19faf58847aabe11261836258d7f",
            "value": "100%"
          }
        },
        "6de4b8a609d54a40b3975f0de9969f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b68fb4e7065b4005bee0659d22c85eea",
            "max": 90,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_831230467dcd4f88a8cd9880a55c9db6",
            "value": 90
          }
        },
        "e0139062262143d5a2564a02421d62d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c86f10a368a349468dfae11f7f41b7a2",
            "placeholder": "​",
            "style": "IPY_MODEL_ed30660335364bfd8e337718d4eae534",
            "value": " 90/90 [00:03&lt;00:00, 37.05it/s]"
          }
        },
        "37fab6e59e9a4b7aa6cbef81442d0b89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3da6f781e1564f77ba031da09d965ece": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21be19faf58847aabe11261836258d7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b68fb4e7065b4005bee0659d22c85eea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "831230467dcd4f88a8cd9880a55c9db6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c86f10a368a349468dfae11f7f41b7a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed30660335364bfd8e337718d4eae534": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}