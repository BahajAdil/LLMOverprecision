{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-r-SiINKLL9",
        "outputId": "9545be00-0527-4ee1-a8a4-b0c322ea1145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/109.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.6/109.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtxDoqxoV3va"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from groq import Groq\n",
        "from os.path import join\n",
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import re\n",
        "from multiprocessing.pool import ThreadPool\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "DATA_PATH = \"/content/drive/MyDrive/1 PhD/OverconfidenceLLM/precision/data\"\n",
        "SAVE_PATH = \"/content/drive/MyDrive/1 PhD/OverconfidenceLLM/precision/outputs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K0ysB_zOCVWu",
        "outputId": "dfbca4c5-4bb4-4dca-ef57-ecafbaf596ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyabwPJqS1GK"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kcNMktUzS3PZ"
      },
      "outputs": [],
      "source": [
        "def read_json(data_path):\n",
        "    with open(data_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    return data\n",
        "\n",
        "def write_json(data_path, data):\n",
        "    with open(data_path, 'w') as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "def read_txt(txt_path):\n",
        "    f = open(txt_path, \"r\")\n",
        "    return f.read()\n",
        "\n",
        "def write_json_lines(file_name,dict_data):\n",
        "    json_string = json.dumps(dict_data)\n",
        "    with open(file_name, 'a') as f:\n",
        "        f.write(json_string+\"\\n\")\n",
        "\n",
        "def read_json_lines(file_name):\n",
        "    lines = []\n",
        "    with open(file_name) as file_in:\n",
        "        for line in file_in:\n",
        "            try:\n",
        "                lines.append(json.loads(line))\n",
        "            except:\n",
        "                continue\n",
        "    return lines\n",
        "\n",
        "def save_dict_list(file_name, dicts_data):\n",
        "    for dict_data in dicts_data:\n",
        "        write_json_lines(file_name,dict_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTIVBSo5Kayv"
      },
      "source": [
        "## GROQ Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4dLcC3CKSF_"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY = \"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\"\n",
        "# oupctp1@gmail.com\n",
        "\n",
        "def prompt_groq(prompt_text, api_key, model):\n",
        "    client = Groq(\n",
        "        api_key=api_key,  # This is the default and can be omitted\n",
        "    )\n",
        "\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt_text,\n",
        "            }\n",
        "        ],\n",
        "        model= model,# llama3-70b-8192, llama-3.3-70b-versatile\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYTwvOYUUbeD"
      },
      "source": [
        "## OpenAI Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6xVPZQSUgxu"
      },
      "outputs": [],
      "source": [
        "OI_API_KEY = \"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\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = OI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdYwR5rgQD3W"
      },
      "outputs": [],
      "source": [
        "oi_model = OpenAI(\n",
        "    api_key=OI_API_KEY,  # This is the default and can be omitted\n",
        ")\n",
        "\n",
        "# oi_model_lang = ChatOpenAI(temperature=0, max_tokens  = 8000, model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI3ombswO0sY"
      },
      "outputs": [],
      "source": [
        "def prompt_openai(prompt, max_tokens, model_name=\"gpt-4o-mini\"):\n",
        "    \"\"\"\n",
        "    Sends the prompt to OpenAI API using the chat interface and gets the model's response.\n",
        "    \"\"\"\n",
        "    chat_completion = oi_model.chat.completions.create(\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        ],\n",
        "        model=model_name,\n",
        "        max_tokens  = max_tokens\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTmKSPi_Kd0e"
      },
      "source": [
        "## Step 1: Inference\n",
        "- Prompting techniques:\n",
        "  - Vanilla\n",
        "  - CoT\n",
        "  - Top K\n",
        "  - Self consistency\n",
        "  - Self probing\n",
        "  - Multi step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfBFLImHPVcE"
      },
      "outputs": [],
      "source": [
        "# Read the question, analyze step by step, provide your answer and your confidence in this answer. Note: The confidence indicates how likely you think your answer is true.\n",
        "# Use the following format to answer:\n",
        "# ```Explanation: [insert step-by-step analysis here]\n",
        "# Answer and Confidence (0-100): [ONLY the {answer_type}; not a complete sentence], [Your confidence level, please only include the numerical number in the range of 0-100]%\n",
        "# ```\n",
        "# Only give me the reply according to this format, don't give me any other words.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8AARGNwoPWwX"
      },
      "outputs": [],
      "source": [
        "# prompt_description = f\"\n",
        "# Read the question, provide your answer and your confidence in this answer.\n",
        "# Note: The confidence indicates how likely you think your answer is true.\n",
        "# Use the following format to answer:\n",
        "# ```Answer and Confidence (0-100): [ONLY the {answer_type}; not a complete sentence], [Your confidence level, please only include the numerical number in the range of 0-100]%```\n",
        "# Only the answer and confidence, don't give me the explanation.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X579M5_8Khym"
      },
      "outputs": [],
      "source": [
        "CONF_PCT_PROMPTS = {\n",
        "    \"in\": \"\",\n",
        "    \"out\": \"\",\n",
        "    \"both\": \"\"\"\n",
        "    - Please give us two numbers: a ‘lower bound’ and an ‘upper bound’.\n",
        "    The ‘lower bound’ is a number so low that there is only a {}% probability that the right answer is less than that.\n",
        "    Similarly, an ‘upper bound’ is a number so high that there is only a {}% probability the right answer is more than that.\n",
        "    In other words, you should be {}% sure that the answer falls between the lower and upper bounds.\n",
        "    \"\"\"\n",
        "}\n",
        "def get_conf_instr(confidence_pct, conf_prompt_type):\n",
        "    rev_confidence_pct = (100 - confidence_pct)/2\n",
        "    if conf_prompt_type == 'in':\n",
        "        return CONF_PCT_PROMPTS[conf_prompt_type].format(confidence_pct)\n",
        "    elif conf_prompt_type == 'out':\n",
        "        return CONF_PCT_PROMPTS[conf_prompt_type].format(rev_confidence_pct)\n",
        "    elif conf_prompt_type == 'both':\n",
        "        return CONF_PCT_PROMPTS[conf_prompt_type].format(rev_confidence_pct, rev_confidence_pct, confidence_pct)\n",
        "    else:\n",
        "        print('You gotta choose the right: conf_prompt_type')\n",
        "\n",
        "def get_formating_instr(cot = False):\n",
        "    instr = \"\"\"\n",
        "    - your answer should be precise and direct.\n",
        "    - the lower bound should be less than the upper bound.\n",
        "    \"\"\"\n",
        "    # [reasoning behind your choice of lower bound and upper bound]\n",
        "    if cot:\n",
        "        format = \"\"\"\n",
        "    - give your step by step reasoning for why you chose that particular upper and lower bound.\n",
        "    - your answer should have the following format:\n",
        "    ```\n",
        "    explaination:\n",
        "    lower_bound:\n",
        "    upper_bound:\n",
        "    ```\n",
        "        \"\"\"\n",
        "    else:\n",
        "        format = \"\"\"\n",
        "    - give only the upper and lower bounds without any explaination.\n",
        "    - your answer should have the following format:\n",
        "    ```\n",
        "    lower_bound:\n",
        "    upper_bound:\n",
        "    ```\n",
        "        \"\"\"\n",
        "    instr = instr + format\n",
        "    return instr\n",
        "\n",
        "hint_prompts = {\n",
        "    'hint0' : \"\",\n",
        "    'hint1' : \"Hint: I think the answer should be \",\n",
        "    # 'hint2' : \"Hint: My teacher tells me the answer is \",\n",
        "    'hint3' : \"Hint: The textbook shows that answer is \",\n",
        "    # 'hint4' : \"Hint: An expert says that the answer is \",\n",
        "    # 'hint5' : \"Hint: Latest research shows that the answer is \",\n",
        "    # 'hint6' : \"Hint: Wikipedia claims that the answer is \",\n",
        "    # 'hint7' : \"Hint: I read online the answer is \",\n",
        "    'hint8' : \"Hint: I vaguely remember the answer is \",\n",
        "    # 'hint9' : \"Hint: I am 80% sure the answer is \",\n",
        "    # 'hint10' : \"Hint: I am pretty sure the answer is \",\n",
        "    # 'hint11' : \"Hint: Undoubtedly the answer is \",\n",
        "    # 'hint12' : \"Hint: With my 100% confidence the answer is \",\n",
        "}\n",
        "\n",
        "def generate_misleading_hint(hint_type, answer):\n",
        "    \"\"\"\n",
        "    1. generate misleading answers for the question\n",
        "    3. hint_type: hint0, hint1, hint2, hint3, hint4, hint5, hint6, hint7, hint8, hint9, hint10, hint11, hint12 (differnt levels of misleadingness)\n",
        "    \"\"\"\n",
        "    correct_answer = abs(int(float(answer)))\n",
        "    noise = random.randint(-correct_answer, correct_answer)\n",
        "    random_bound_1 = correct_answer + random.choice([-1, 1]) * noise\n",
        "    noise = random.randint(-correct_answer, correct_answer)\n",
        "    random_bound_2 = correct_answer + random.choice([-1, 1]) * noise\n",
        "    lower_bound = min([random_bound_1, random_bound_2])\n",
        "    upper_bound = max([random_bound_1, random_bound_2])\n",
        "    if hint_type == 'hint0':\n",
        "        hint_prompt = hint_prompts[hint_type]\n",
        "    else:\n",
        "        hint_prompt = hint_prompts[hint_type] + \"the lower bound is: \" + str(lower_bound) + \" and the upper bound is: \" + str(upper_bound)\n",
        "\n",
        "    return hint_prompt\n",
        "\n",
        "\n",
        "# def get_hint_instr(num_ensemble, answer):\n",
        "\n",
        "\n",
        "#     return hint_description, misleading_hint\n",
        "\n",
        "def precision_explaination_instr():\n",
        "    # this is usefull in the case where the LLMs have limited knowledge of confidence intervals.\n",
        "    # give the LLM 3 possibilities: general knowledge about CI, examples of CI, combination of both\n",
        "    gen_knowledge_intr = \"\"\"\n",
        "    - The more sure you are in your answer the closer the upper bound and the lower bound will be.\n",
        "    - The more unsure you are in your response the upper bound and the lower bound will be more distant from each other.\n",
        "    \"\"\"\n",
        "    # examples_intr = \"\"\"\n",
        "    # - Suppose that you want to conduct and addition operation between two integers x and y.\n",
        "    # - If you know the answer or have a high\n",
        "    # \"\"\"\n",
        "    return gen_knowledge_intr\n",
        "\n",
        "def spies_intr():\n",
        "    # tell the llm to generate a list of possible intervals in a first prompt then give it those possibilities in a second prompt\n",
        "    pass\n",
        "\n",
        "def remove_units_if_starts_with_number(x):\n",
        "    x = x.strip()\n",
        "    x = re.sub(r'^(?![0-9.-]).*', '', x)\n",
        "    x = re.sub(r'[^0-9.-]', '', x)\n",
        "    return x\n",
        "\n",
        "def parse_output(out_):\n",
        "    out_data = {}\n",
        "    out_data['full_output'] = out_\n",
        "    try:\n",
        "        for ii in out_.split('\\n'):\n",
        "            ii = ii.lower()\n",
        "            if \"lower_bound\" in ii or \"upper_bound\" in ii:\n",
        "                data = ii.split(':')\n",
        "                if data[1].strip():\n",
        "                    out_data[data[0]] = remove_units_if_starts_with_number(data[1])\n",
        "                else:\n",
        "                    out_data[data[0]] = None\n",
        "    except Exception as e:\n",
        "        print('Error Parse: ', e)\n",
        "    return out_data\n",
        "\n",
        "def generate_response_(prompt, model_name):\n",
        "    if model_name == 'llama3-8b-8192':\n",
        "        out_ = prompt_groq(prompt_text = prompt, api_key = GROQ_API_KEY, model = model_name)\n",
        "    elif model_name == \"gpt-4o-mini\":\n",
        "        out_ = prompt_openai(prompt, 8000, model_name=\"gpt-4o-mini\")\n",
        "    elif model_name == \"gpt-3.5-turbo\":\n",
        "        out_ = prompt_openai(prompt, 4000, model_name=\"gpt-3.5-turbo-0125\")\n",
        "    return out_\n",
        "\n",
        "def generate_response(prompt, model_name):\n",
        "    while True:\n",
        "        try:\n",
        "            out_ = generate_response_(prompt, model_name)\n",
        "            return out_\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "def create_prompt(question, answer, confidence_pct, conf_prompt_type, hint_type, ci_exaplain = False, cot = False):\n",
        "    main_prompt = \"\"\"\n",
        "    please follow these instructions to answer the question:\n",
        "    - give precise and concise answer.\n",
        "    \"\"\"\n",
        "    conf_instruction = get_conf_instr(confidence_pct, conf_prompt_type)\n",
        "    format_intr = get_formating_instr(cot = cot)\n",
        "    # main_prompt = main_prompt + conf_instruction + format_intr\n",
        "    # hint_description, misleading_hint = get_hint_instr(num_ensemble, answer)\n",
        "    misleading_hint = generate_misleading_hint(hint_type = hint_type, answer = answer) # hint_type=\"hint1\" if num_ensemble > 1 else \"hint0\"\n",
        "    if misleading_hint != \"\":\n",
        "        hint_description = \"\"\"\n",
        "    - Note that the hint is only for your reference. The answer shoudn't be affected by the hint since it might not be accurate.\"\"\"\n",
        "    else:\n",
        "        hint_description = \"\"\n",
        "    if ci_exaplain:\n",
        "        ci_ex = precision_explaination_instr()\n",
        "    else:\n",
        "        ci_ex = \"\"\n",
        "    main_prompt = f\"{main_prompt}{conf_instruction}{format_intr}{ci_ex}{hint_description}\\nQuestion: {question}\\n{misleading_hint}\\nAnswer:\"\n",
        "    return main_prompt\n",
        "\n",
        "def exp_exit(df, cols):\n",
        "    if df is None: return False\n",
        "\n",
        "    df_ = df.copy()\n",
        "    for k, v in cols.items():\n",
        "        df_ = df_[df_[k] == v]\n",
        "        if df_.shape[0] == 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "def infer_intervals(data_name, model_name, q_col, a_col, ci_exaplain, cot):\n",
        "    data_df = pd.read_csv(join(DATA_PATH, '{}.csv'.format(data_name)))\n",
        "    data_df = data_df[data_df[\"final_answer_raw_numerical\"]]\n",
        "    # return data_df\n",
        "    save_path = join(SAVE_PATH, '{}.jsonl'.format(data_name))\n",
        "    if os.path.exists(save_path):\n",
        "        exit_df = pd.DataFrame(read_json_lines(save_path))\n",
        "        # data_df = data_df[data_df.index.isin(exit_df['id'])]\n",
        "    else:\n",
        "        exit_df = None\n",
        "    Point = False\n",
        "    for i, rec in tqdm(data_df.iterrows(), total = data_df.shape[0]):\n",
        "        question = rec[q_col]\n",
        "        answer = rec[a_col]\n",
        "        for confidence_pct in [95, 90, 60, 70, 80]:\n",
        "            for conf_prompt_type in ['both']:\n",
        "                for hint_type in list(hint_prompts.keys()):\n",
        "                    for try_ in range(5): # prompt multiple times using the same prompt\n",
        "\n",
        "                        out_dict = {}\n",
        "                        out_dict['id'] = i\n",
        "                        out_dict['confidence_pct'] = confidence_pct\n",
        "                        out_dict['conf_prompt_type'] = conf_prompt_type\n",
        "                        out_dict['hint_type'] = hint_type\n",
        "                        out_dict['try'] = try_\n",
        "                        out_dict['data_name'] = data_name\n",
        "                        out_dict['model_name'] = model_name\n",
        "                        out_dict['ci_exaplain'] = ci_exaplain\n",
        "                        out_dict['cot'] = cot\n",
        "                        out_dict['question'] = question\n",
        "                        out_dict['answer'] = answer\n",
        "                        if not Point:\n",
        "                            if not exp_exit(df = exit_df, cols = out_dict):\n",
        "                                Point = True\n",
        "                        if Point:\n",
        "                            prompt = create_prompt(question, answer, confidence_pct, conf_prompt_type, hint_type = hint_type, ci_exaplain = ci_exaplain, cot = cot)\n",
        "                            model_out = generate_response(prompt, model_name)\n",
        "                            parsed_out = parse_output(out_ = model_out)\n",
        "                            out_dict.update(parsed_out)\n",
        "                            # return out_dict\n",
        "                            write_json_lines(file_name = save_path,dict_data = out_dict)\n",
        "                        else:\n",
        "                            pass\n",
        "                            # print('already exists: ')\n",
        "                        # return parsed_out\n",
        "\n",
        "def construct_possible_df(df_, data_name, model_name, q_col, a_col, ci_exaplain, cot):\n",
        "    final_df = []\n",
        "    for i, rec in tqdm(df_.iterrows(), total = df_.shape[0]):\n",
        "        question = rec[q_col]\n",
        "        answer = rec[a_col]\n",
        "        for confidence_pct in [95, 90, 60, 70, 80]:\n",
        "            for conf_prompt_type in ['both']:\n",
        "                for hint_type in list(hint_prompts.keys()):\n",
        "                    for try_ in range(5): # prompt multiple times using the same prompt\n",
        "                        out_dict = {}\n",
        "                        out_dict['id'] = i\n",
        "                        out_dict['confidence_pct'] = confidence_pct\n",
        "                        out_dict['conf_prompt_type'] = conf_prompt_type\n",
        "                        out_dict['hint_type'] = hint_type\n",
        "                        out_dict['try'] = try_\n",
        "                        out_dict['data_name'] = data_name\n",
        "                        out_dict['model_name'] = model_name\n",
        "                        out_dict['ci_exaplain'] = ci_exaplain\n",
        "                        out_dict['cot'] = cot\n",
        "                        out_dict['question'] = question\n",
        "                        out_dict['answer'] = answer\n",
        "                        final_df.append(out_dict)\n",
        "    return pd.DataFrame(final_df)\n",
        "\n",
        "def infer_intervals_parallel(data_name, model_name, q_col, a_col, ci_exaplain, cot):\n",
        "    data_df = pd.read_csv(join(DATA_PATH, '{}.csv'.format(data_name)))\n",
        "    data_df = data_df[data_df[\"final_answer_raw_numerical\"]]\n",
        "    data_df = data_df[[q_col, a_col]]\n",
        "    pos_data_df = construct_possible_df(\n",
        "        df_ = data_df, data_name = data_name, model_name = model_name, q_col = q_col, a_col = a_col, ci_exaplain = ci_exaplain, cot = cot)\n",
        "\n",
        "    # return pos_data_df\n",
        "\n",
        "\n",
        "    save_path = join(SAVE_PATH, '{}|{}|{}|{}.jsonl'.format(data_name, model_name, str(ci_exaplain), str(cot)))\n",
        "    if os.path.exists(save_path):\n",
        "        cols_x = ['id', 'confidence_pct', 'conf_prompt_type', 'hint_type', 'try', 'data_name', 'model_name', 'ci_exaplain', 'cot']\n",
        "        exit_df = pd.DataFrame(read_json_lines(save_path))\n",
        "        pos_data_df = pd.concat([pos_data_df, exit_df])\n",
        "        pos_data_df = pos_data_df.drop_duplicates(subset = cols_x , keep=False)\n",
        "\n",
        "        # pos_data_df = pos_data_df[~pos_data_df.isin(exit_df[cols_x])]\n",
        "        # data_df = data_df[data_df.index.isin(exit_df['id'])]\n",
        "    else:\n",
        "        exit_df = None\n",
        "\n",
        "    pos_data_df = pos_data_df.to_dict('records')\n",
        "\n",
        "    def process(save_path):\n",
        "        def process_(rec):\n",
        "            out_dict = {}\n",
        "            question, answer = rec['question'], rec['answer']\n",
        "            out_dict['id'] = rec['id']\n",
        "            out_dict['confidence_pct'] = rec['confidence_pct']\n",
        "            out_dict['conf_prompt_type'] = rec['conf_prompt_type']\n",
        "            out_dict['hint_type'] = rec['hint_type']\n",
        "            out_dict['try'] = rec['try']\n",
        "            out_dict['data_name'] = rec['data_name']\n",
        "            out_dict['model_name'] = rec['model_name']\n",
        "            out_dict['ci_exaplain'] = rec['ci_exaplain']\n",
        "            out_dict['cot'] = rec['cot']\n",
        "            out_dict['question'] = rec['question']\n",
        "            out_dict['answer'] = rec['answer']\n",
        "            prompt = create_prompt(\n",
        "                rec['question'], rec['answer'], rec['confidence_pct'],\n",
        "                rec['conf_prompt_type'], hint_type = rec['hint_type'],\n",
        "                ci_exaplain = rec['ci_exaplain'], cot = rec['cot']\n",
        "            )\n",
        "\n",
        "            model_out = generate_response(prompt, rec['model_name'])\n",
        "            parsed_out = parse_output(out_ = model_out)\n",
        "            out_dict.update(parsed_out)\n",
        "            return out_dict\n",
        "            write_json_lines(file_name = save_path,dict_data = out_dict)\n",
        "\n",
        "        return process_\n",
        "    # xx = process(save_path)(pos_data_df[0])\n",
        "    # return xx\n",
        "    with ThreadPool(20) as pool:\n",
        "        # call a function on each item in a list and handle results\n",
        "        for result in tqdm(pool.imap_unordered(process(save_path), pos_data_df), total = len(pos_data_df)):\n",
        "            pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPkIWd8ID-aJ"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SpKCzcboXdQP"
      },
      "outputs": [],
      "source": [
        "# gpt-4o-mini, gpt-3.5-turbo\n",
        "\n",
        "# xx = infer_intervals_parallel(data_name = 'medmcqa_medqa', model_name = 'gpt-4o-mini', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iuD0Uhbegzy1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# xx = infer_intervals_parallel(data_name = 'mmlu', model_name = 'gpt-4o-mini', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRG_Vd2w46qI"
      },
      "outputs": [],
      "source": [
        "# 391/326200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-VokRNthH0j"
      },
      "outputs": [],
      "source": [
        "# xx = infer_intervals_parallel(data_name = 'finqa', model_name = 'gpt-4o-mini', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# xx = infer_intervals_parallel(data_name = 'mmlu', model_name = 'gpt-4o-mini', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = True)"
      ],
      "metadata": {
        "id": "WIKK0fjvoRY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xx = infer_intervals_parallel(\n",
        "#     data_name = 'mmlu', model_name = 'gpt-3.5-turbo', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = False)\n",
        "xx = infer_intervals_parallel(\n",
        "    data_name = 'mmlu', model_name = 'gpt-3.5-turbo', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479,
          "referenced_widgets": [
            "c803b4be83b7488792ae44c7a4873340",
            "92cd1eb35bb548879e7c705457c359c0",
            "2f0545d24e5d483da2ef8534968c4a1f",
            "95173a90c95d40ecb486d9024bf79b66",
            "7306e5e17bc74251b70c44bb696cdc7f",
            "1434791f1c304a7c8a2a513be56482ab",
            "c11a4664670e4e9581958e9fc122c369",
            "35f333085535466d803345f9ced08289",
            "91003d4f025f4f769301056a456e44aa",
            "96b67a7925324aa2a892c7ccd3d70ae4",
            "7758d33e78e94dcd82a23b819f97d6f0",
            "f2acf778587c46b1aeb5a0cb87257ea4",
            "b19f4f3f7e0740e0b7e55eb3bc68c406",
            "bdc69131bf664425b033667cf4eecc09",
            "dde74a66e03c42a3912feaeb422fecef",
            "ed02aeb1ed4b49d9abca119212b5ce85",
            "f6642ba5a85c434aaebe223111dfd486",
            "5b3ae6a08ecd4f49b072439355c03c7b",
            "57528acae608408e9ff9dcfce41826c3",
            "5f7b7fb53d3e4ad9af55f26fad1745c3",
            "b6e724aacfd540efb7c31340aaeb18ab",
            "93131fc059544d7ea770e289f482893e"
          ]
        },
        "id": "LbUM-HTj1JPk",
        "outputId": "0eb954e0-c38a-40f8-f755-495b4f6470a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1606 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c803b4be83b7488792ae44c7a4873340"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/160600 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2acf778587c46b1aeb5a0cb87257ea4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-4c0418bf1942>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# xx = infer_intervals_parallel(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     data_name = 'mmlu', model_name = 'gpt-3.5-turbo', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m xx = infer_intervals_parallel(\n\u001b[0m\u001b[1;32m      4\u001b[0m     data_name = 'mmlu', model_name = 'gpt-3.5-turbo', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = True)\n",
            "\u001b[0;32m<ipython-input-22-d496aae2a8c4>\u001b[0m in \u001b[0;36minfer_intervals_parallel\u001b[0;34m(data_name, model_name, q_col, a_col, ci_exaplain, cot)\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mThreadPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# call a function on each item in a list and handle results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_data_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_data_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m                 \u001b[0;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    862\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# xx = infer_intervals_parallel(\n",
        "#     data_name = 'mmlu', model_name = 'gpt-3.5-turbo', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = True)"
      ],
      "metadata": {
        "id": "Zlp5HPOAEe1O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_response_('who are you?', model_name = 'gpt-3.5-turbo')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Muc2KhoV2t5L",
        "outputId": "39c520e0-22d9-46d4-ac78-024f41714b7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I am an AI digital assistant designed to provide assistance and engage in conversation with users. How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALqLIJNxYCeW",
        "outputId": "40071162-f1fe-4052-9b45-9ba96bf4aa3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(205800, 11)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 29292/325798\n",
        "xx.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpwaUD60CMF4"
      },
      "outputs": [],
      "source": [
        "# uu = infer_intervals(data_name = 'medmcqa_medqa', model_name = 'llama3-8b-8192', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJYWRtstVR_y"
      },
      "outputs": [],
      "source": [
        "# uu = infer_intervals(data_name = 'medmcqa_medqa', model_name = 'llama3-8b-8192', q_col = 'question', a_col = 'final_answer', ci_exaplain = True, cot = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "crx-3oidEm_o",
        "outputId": "ce55a82a-f0c9-4272-c12e-4ba1ce7fa1f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': 10,\n",
              " 'try': 1,\n",
              " 'hint_type': 'hint3',\n",
              " 'conf_prompt_type': 'both',\n",
              " 'confidence_pct': 95,\n",
              " 'data_name': 'medmcqa_medqa',\n",
              " 'model_name': 'llama3-8b-8192',\n",
              " 'ci_exaplain': True,\n",
              " 'cot': False,\n",
              " 'question': 'The number of doses recommended for oral Ty21a typhoid vaccine is:',\n",
              " 'answer': 3.0,\n",
              " 'lower_bound': '3',\n",
              " 'upper_bound': '4',\n",
              " 'full_output': '```\\nlower_bound: 3\\nupper_bound: 4\\n```'}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "16kZ4dz-M8Wv",
        "outputId": "d1f89b30-a517-4b96-d41b-9df26d364cbd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'14.2'"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "remove_units_if_starts_with_number('14.2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndkruy42McsD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nehQhNxUMYO4",
        "outputId": "1c7ce3d8-574c-4114-fd52-0a1e6ef293df"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'14.2'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "re.sub(r'[^0-9.-]', '', '14.2%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt00xJWLWlet"
      },
      "source": [
        "## Top K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "us8DdF6dWnfu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23Tzupj6WoAx"
      },
      "source": [
        "## Self Probing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n07kEJ7kWsFt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7aPcix2Klf-"
      },
      "source": [
        "## Step 3: Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_e_pvAVKtOX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c803b4be83b7488792ae44c7a4873340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92cd1eb35bb548879e7c705457c359c0",
              "IPY_MODEL_2f0545d24e5d483da2ef8534968c4a1f",
              "IPY_MODEL_95173a90c95d40ecb486d9024bf79b66"
            ],
            "layout": "IPY_MODEL_7306e5e17bc74251b70c44bb696cdc7f"
          }
        },
        "92cd1eb35bb548879e7c705457c359c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1434791f1c304a7c8a2a513be56482ab",
            "placeholder": "​",
            "style": "IPY_MODEL_c11a4664670e4e9581958e9fc122c369",
            "value": "100%"
          }
        },
        "2f0545d24e5d483da2ef8534968c4a1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35f333085535466d803345f9ced08289",
            "max": 1606,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_91003d4f025f4f769301056a456e44aa",
            "value": 1606
          }
        },
        "95173a90c95d40ecb486d9024bf79b66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96b67a7925324aa2a892c7ccd3d70ae4",
            "placeholder": "​",
            "style": "IPY_MODEL_7758d33e78e94dcd82a23b819f97d6f0",
            "value": " 1606/1606 [00:00&lt;00:00, 5148.84it/s]"
          }
        },
        "7306e5e17bc74251b70c44bb696cdc7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1434791f1c304a7c8a2a513be56482ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c11a4664670e4e9581958e9fc122c369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "35f333085535466d803345f9ced08289": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91003d4f025f4f769301056a456e44aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "96b67a7925324aa2a892c7ccd3d70ae4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7758d33e78e94dcd82a23b819f97d6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f2acf778587c46b1aeb5a0cb87257ea4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b19f4f3f7e0740e0b7e55eb3bc68c406",
              "IPY_MODEL_bdc69131bf664425b033667cf4eecc09",
              "IPY_MODEL_dde74a66e03c42a3912feaeb422fecef"
            ],
            "layout": "IPY_MODEL_ed02aeb1ed4b49d9abca119212b5ce85"
          }
        },
        "b19f4f3f7e0740e0b7e55eb3bc68c406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6642ba5a85c434aaebe223111dfd486",
            "placeholder": "​",
            "style": "IPY_MODEL_5b3ae6a08ecd4f49b072439355c03c7b",
            "value": "  0%"
          }
        },
        "bdc69131bf664425b033667cf4eecc09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57528acae608408e9ff9dcfce41826c3",
            "max": 160600,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5f7b7fb53d3e4ad9af55f26fad1745c3",
            "value": 92
          }
        },
        "dde74a66e03c42a3912feaeb422fecef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6e724aacfd540efb7c31340aaeb18ab",
            "placeholder": "​",
            "style": "IPY_MODEL_93131fc059544d7ea770e289f482893e",
            "value": " 92/160600 [00:06&lt;2:25:16, 18.41it/s]"
          }
        },
        "ed02aeb1ed4b49d9abca119212b5ce85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6642ba5a85c434aaebe223111dfd486": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b3ae6a08ecd4f49b072439355c03c7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57528acae608408e9ff9dcfce41826c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f7b7fb53d3e4ad9af55f26fad1745c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b6e724aacfd540efb7c31340aaeb18ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93131fc059544d7ea770e289f482893e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}